{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phishing Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ektanegi25/Phishing-Classifier/main/Phishing_Legitimate_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the rows and columns of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping 'id' column because it is not required\n",
    "df.drop(labels='id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing sweetviz library to analyze and visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sweetviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sweetviz as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report = sv.analyze(df)\n",
    "#report.show_html(\"report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install autoviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from autoviz.AutoViz_Class import AutoViz_Class\n",
    "#AV = AutoViz_Class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto_report = AV.AutoViz(\"https://raw.githubusercontent.com/ektanegi25/Phishing-Classifier/main/Phishing_Legitimate_full.csv\", depVar='CLASS_LABEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see information of Data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing Dtype from int64, float64 to int32, float32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_cols = df.select_dtypes('float64').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in float_cols:\n",
    "    df[cols] = df[cols].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols = df.select_dtypes('int64').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in int_cols:\n",
    "    df[cols] = df[cols].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converted 64bits to 32bits\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming label column from CLASS_LABEL to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'CLASS_LABEL':'Labels'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some random sample from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Statistics of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Labels'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making function to see correlation of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_heatmap(data,idx_s,idx_e):\n",
    "    \n",
    "    y = data['Labels']\n",
    "    \n",
    "    temp = data.iloc[:,idx_s:idx_e]\n",
    "    \n",
    "    if 'id' in temp.columns:\n",
    "        del temp['id']\n",
    "        \n",
    "    temp['Labels'] = y\n",
    "    \n",
    "    sns.heatmap(temp.corr(),annot=True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "pd.set_option('display.max_columns',None)\n",
    "#plt.rcParams['figure.figsize'] = (15,10)\n",
    "corr_heatmap(df,0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_heatmap(df,10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_heatmap(df,20,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_heatmap(df,30,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn for feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(labels='Labels',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[['Labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discrete_features = X.dtypes == int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_score = mutual_info_classif(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_score = pd.Series(mi_score,name=\"Mi Score\",index = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_score = mi_score.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mi_scores(scores):\n",
    "    \n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width,scores)\n",
    "    plt.yticks(width,ticks)\n",
    "    plt.title(\"MI Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plot_mi_scores(mi_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic(data,top_n):\n",
    "    \n",
    "    top_n_features = mi_score.sort_values(ascending=False).head(top_n).index.tolist()\n",
    "    \n",
    "    X = data[top_n_features]\n",
    "    y = data['Labels']\n",
    "    \n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,shuffle=True)\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred = lr.predict(X_test)\n",
    "    \n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    \n",
    "    return precision,accuracy,recall,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20,51):\n",
    "    \n",
    "    precision,accuracy,recall,f1 = train_logistic(df,i)\n",
    "    \n",
    "    log_scores.append([i,precision,accuracy,recall,f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting scores in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_log = pd.DataFrame(log_scores,columns=['top_features','precision','accuracy','recall','f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a plot of performance score for better decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_dict = {\n",
    "    'family': 'serif',  # Font family ('serif', 'sans-serif', 'monospace', etc.)\n",
    "    'style': 'italic',  # Font style ('normal', 'italic', 'oblique')\n",
    "    'weight': 'bold',   # Font weight ('normal', 'bold', 'heavy', 'light', etc.)\n",
    "    'size': 20,         # Font size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=score_log,x='top_features',y='precision',label='Precision Score')\n",
    "sns.lineplot(data=score_log,x='top_features',y='recall',label='recall Score')\n",
    "sns.lineplot(data=score_log,x='top_features',y='f1',label='fi Score')\n",
    "sns.lineplot(data=score_log,x='top_features',y='accuracy',label='accuracy Score')\n",
    "\n",
    "plt.grid(True,linestyle='--',alpha=0.6)\n",
    "plt.xlabel('Top Features',fontdict=font_dict)\n",
    "plt.ylabel('Scores',fontdict=font_dict)\n",
    "plt.xticks(list(range(20,51)),fontsize=15)\n",
    "plt.title('Performance Metrics of logistic Regression',fontdict=font_dict)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can we see selecting top 27 or 28 features can give a good score of all performance metrics\n",
    "# This Scores are given by Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rfc(data,top_n):\n",
    "    \n",
    "    top_n_features = mi_score.sort_values(ascending=False).head(top_n).index.tolist()\n",
    "    \n",
    "    X = data[top_n_features]\n",
    "    y = data['Labels']\n",
    "    \n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,shuffle=True)\n",
    "    \n",
    "    rfc = RandomForestClassifier(n_estimators=200,criterion='entropy',max_depth=32,max_features=1.0)\n",
    "    rfc.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred = rfc.predict(X_test)\n",
    "    \n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    \n",
    "    return precision,accuracy,recall,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20,51):\n",
    "    \n",
    "    precision,accuracy,recall,f1 = train_rfc(df,i)\n",
    "    print(f'{i} Precision:{precision} Accuracy:{accuracy} Recall:{recall} F1:{f1}')\n",
    "    rfc_scores.append([precision,accuracy,recall,f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting rfc scores into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_rfc = pd.DataFrame(rfc_scores,columns=['top_features','precision','accuracy','recall','f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a plot of performance score for better decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=score_log,x='top_features',y='precision',label='Precision Score')\n",
    "sns.lineplot(data=score_log,x='top_features',y='recall',label='recall Score')\n",
    "sns.lineplot(data=score_log,x='top_features',y='f1',label='fi Score')\n",
    "sns.lineplot(data=score_log,x='top_features',y='accuracy',label='accuracy Score')\n",
    "\n",
    "plt.grid(True,linestyle='--',alpha=0.6)\n",
    "plt.xlabel('Top Features',fontdict=font_dict)\n",
    "plt.ylabel('Scores',fontdict=font_dict)\n",
    "plt.xticks(list(range(20,51)),fontsize=15)\n",
    "plt.title('Performance Metrics',fontdict=font_dict)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can we see selecting top 27 or 28 features can give a good score of all performance metrics\n",
    "# This Scores are given by Random Forest Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
